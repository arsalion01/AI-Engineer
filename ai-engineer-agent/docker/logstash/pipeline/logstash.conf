# AI Engineer Agent - Logstash Configuration
# Processes application logs and sends them to Elasticsearch

input {
  # Docker logs via beats
  beats {
    port => 5044
  }
  
  # Direct log files
  file {
    path => "/var/log/app/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    type => "application"
  }
  
  # Nginx access logs
  file {
    path => "/var/log/nginx/access.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    type => "nginx_access"
  }
  
  # Nginx error logs
  file {
    path => "/var/log/nginx/error.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    type => "nginx_error"
  }
  
  # Application JSON logs
  tcp {
    port => 5000
    type => "json_logs"
    codec => json_lines
  }
}

filter {
  # Remove empty fields
  if [message] == "" {
    drop { }
  }
  
  # Parse application logs
  if [type] == "application" {
    # Parse JSON logs
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
      }
      
      # Parse timestamp if present
      if [timestamp] {
        date {
          match => [ "timestamp", "ISO8601" ]
        }
      }
      
      # Extract log level
      if [level] {
        mutate {
          uppercase => [ "level" ]
        }
      }
      
      # Add application context
      mutate {
        add_field => { "service" => "ai-engineer-app" }
        add_field => { "environment" => "${NODE_ENV:development}" }
      }
    } else {
      # Parse structured logs with grok
      grok {
        match => { 
          "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:level}\] %{DATA:component}: %{GREEDYDATA:log_message}"
        }
      }
      
      date {
        match => [ "timestamp", "yyyy-MM-dd HH:mm:ss.SSS" ]
      }
    }
  }
  
  # Parse Nginx access logs
  if [type] == "nginx_access" {
    grok {
      match => { 
        "message" => "%{NGINXACCESS}"
      }
    }
    
    # Convert numeric fields
    mutate {
      convert => { 
        "response" => "integer"
        "bytes" => "integer"
        "responsetime" => "float"
      }
    }
    
    # Parse user agent
    if [agent] != "-" {
      useragent {
        source => "agent"
        target => "user_agent"
      }
    }
    
    # GeoIP lookup for client IP
    if [clientip] !~ /^(10\.|192\.168\.|172\.(1[6-9]|2\d|3[01])\.))/ {
      geoip {
        source => "clientip"
        target => "geoip"
      }
    }
    
    # Add service context
    mutate {
      add_field => { "service" => "nginx" }
    }
  }
  
  # Parse Nginx error logs
  if [type] == "nginx_error" {
    grok {
      match => {
        "message" => "(?<timestamp>%{YEAR}[./-]%{MONTHNUM}[./-]%{MONTHDAY}[- ]%{TIME}) \[%{LOGLEVEL:severity}\] %{POSINT:pid}#%{NUMBER}: %{GREEDYDATA:error_message}"
      }
    }
    
    date {
      match => [ "timestamp", "yyyy/MM/dd HH:mm:ss" ]
    }
    
    mutate {
      add_field => { "service" => "nginx" }
    }
  }
  
  # Parse WebSocket server logs
  if [fields][service] == "websocket-server" {
    json {
      source => "message"
    }
    
    mutate {
      add_field => { "service" => "websocket-server" }
    }
  }
  
  # Parse database logs
  if [fields][service] =~ /postgres|timescaledb/ {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{POSINT:pid}\] %{WORD:severity}: %{GREEDYDATA:db_message}"
      }
    }
    
    date {
      match => [ "timestamp", "yyyy-MM-dd HH:mm:ss.SSS ZZZ" ]
    }
  }
  
  # Common field processing
  # Remove sensitive information
  mutate {
    gsub => [
      "message", "password=[^\s]+", "password=***",
      "message", "token=[^\s]+", "token=***",
      "message", "key=[^\s]+", "key=***",
      "message", "secret=[^\s]+", "secret=***"
    ]
  }
  
  # Add common fields
  mutate {
    add_field => { "[@metadata][index_name]" => "ai-engineer-agent-%{+YYYY.MM.dd}" }
  }
  
  # Tag errors and warnings
  if [level] in ["ERROR", "FATAL"] or [severity] == "error" {
    mutate {
      add_tag => [ "error" ]
    }
  }
  
  if [level] == "WARN" or [severity] == "warn" {
    mutate {
      add_tag => [ "warning" ]
    }
  }
  
  # Performance monitoring
  if [response] and [response] >= 500 {
    mutate {
      add_tag => [ "http_5xx" ]
    }
  }
  
  if [responsetime] and [responsetime] > 5.0 {
    mutate {
      add_tag => [ "slow_request" ]
    }
  }
  
  # Remove fields we don't need
  mutate {
    remove_field => [ "host", "@version" ]
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index_name]}"
    template_name => "ai-engineer-agent"
    template => "/usr/share/logstash/templates/ai-engineer-agent.json"
    template_overwrite => true
  }
  
  # Debug output (remove in production)
  if [level] == "DEBUG" {
    stdout {
      codec => rubydebug {
        metadata => true
      }
    }
  }
  
  # Send errors to dead letter queue for investigation
  if "_grokparsefailure" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "ai-engineer-agent-failed-%{+YYYY.MM.dd}"
    }
  }
}